{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1aKWKl4LFer"
   },
   "source": [
    "# Keyword Detection on Websites\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t74wPcuOLaVW"
   },
   "source": [
    "## Assignment\n",
    "Your task is to create an algorithm, that takes html page as input and infers if the page contains the information about cancer tumorboard or not. What is a tumor board? Tumor Board is a consilium of doctors (usually from different disciplines) discussing cancer cases in their departments. If you want to know more please read this article.\n",
    "\n",
    "The expected result is a CSV file for test data with columns [doc_id and prediction].\n",
    "\n",
    "Bonus: if you would like to go the extra mile in this task try to identify tumor board types interdisciplinary, breast, and any third type of tumor board up to you. For these tumor boards please try to identify their schedule: Day (e.g. Friday), frequency (e.g. weekly, bi-weekly, monthly), and time when they start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VDsPJHuLert"
   },
   "source": [
    "## Data Description\n",
    "You have train.csv and test.csv files and folder with corresponding .html files.\n",
    "\n",
    "Files:\n",
    "\n",
    "train.csv contains next columns: url, doc_id and label\n",
    "test.csv contains next columns: url and doc_id\n",
    "htmls contains files with names {doc_id}.html\n",
    "keyword2tumor_type.csv contains useful keywords for types of tumorboards\n",
    "Description of tumor board labels:\n",
    "\n",
    "1 (no evidence): tumor boards are not mentioned on the page\n",
    "2 (medium confidence): tumor boards are mentioned, but the page is not completely dedicated to tumor board description\n",
    "3 (high confidence): page is completely dedicated to the description of tumor board types and dates\n",
    "You are asked to prepare a model using htmls, referred to in train.csv, and make predictions for htmls from test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYlqDlezLi6C"
   },
   "source": [
    "## Practicalities\n",
    "You should prepare a Jupyter Notebook with the code that you used for making the predictions and the following documentation:\n",
    "\n",
    "How did you decide to handle this amount of data?\n",
    "How did you decide to do feature engineering?\n",
    "How did you decide which models to try (if you decide to train any models)?\n",
    "How did you perform validation of your model?\n",
    "What metrics did you measure?\n",
    "How do you expect your model to perform on test data (in terms of your metrics)?\n",
    "How fast will your algorithm performs and how could you improve its performance if you would have more time?\n",
    "How do you think you would be able to improve your algorithm if you would have more data?\n",
    "What potential issues do you see with your algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNVf9nfELm8h"
   },
   "source": [
    "## Tips\n",
    "to extract clean text from the page you can use BeautifulSoup module like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzZZ1avgLsxh"
   },
   "source": [
    "from bs import BeautifulSoup\n",
    "\n",
    "content = read_html()\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "clean_text = soup.get_text(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9WAtI_zL0Xa"
   },
   "source": [
    "\n",
    "## If you decide that you don't need, for example, tags <p> in your document you can do this:##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQbiQRdYMA57"
   },
   "source": [
    "from bs import BeautifulSoup\n",
    "\n",
    "content = read_html()\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "for tag in soup.find_all('p'):\n",
    "    tag.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajUGGOYbNVrl"
   },
   "source": [
    "#### To download the dataset <a href=\"https://drive.google.com/drive/folders/1Qs2fLj9HmAzx2YGKmqkePCa1Acs5JY3Z?usp=sharing\"> Click here </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     100 non-null    object\n",
      " 1   doc_id  100 non-null    int64 \n",
      " 2   label   100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n",
      "None\n",
      "\n",
      "Test CSV:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48 entries, 0 to 47\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     48 non-null     object\n",
      " 1   doc_id  48 non-null     int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 900.0+ bytes\n",
      "None\n",
      "\n",
      "Keyword CSV:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126 entries, 0 to 125\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   keyword     126 non-null    object\n",
      " 1   tumor_type  126 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "Sample Data from Train CSV:\n",
      "                                                 url  doc_id  label\n",
      "0  http://elbe-elster-klinikum.de/fachbereiche/ch...       1      1\n",
      "1  http://klinikum-bayreuth.de/einrichtungen/zent...       3      3\n",
      "2  http://klinikum-braunschweig.de/info.php/?id_o...       4      1\n",
      "3  http://klinikum-braunschweig.de/info.php/?id_o...       5      1\n",
      "4  http://klinikum-braunschweig.de/zuweiser/tumor...       6      3\n",
      "\n",
      "Sample Data from Test CSV:\n",
      "                                                 url  doc_id\n",
      "0  http://chirurgie-goettingen.de/medizinische-ve...       0\n",
      "1  http://evkb.de/kliniken-zentren/chirurgie/allg...       2\n",
      "2  http://krebszentrum.kreiskliniken-reutlingen.d...       7\n",
      "3  http://marienhospital-buer.de/mhb-av-chirurgie...      15\n",
      "4  http://marienhospital-buer.de/mhb-av-chirurgie...      16\n",
      "\n",
      "Sample Data from Keyword CSV:\n",
      "        keyword tumor_type\n",
      "0  senologische      Brust\n",
      "1  brustzentrum      Brust\n",
      "2        breast      Brust\n",
      "3        thorax      Brust\n",
      "4     thorakale      Brust\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read train and test CSV files\n",
    "train_df = pd.read_csv(\"C:\\\\Users\\\\manoj\\\\Downloads\\\\train.csv\")\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\manoj\\\\Downloads\\\\test.csv\")\n",
    "\n",
    "# Read keyword file\n",
    "keyword_df = pd.read_csv(\"C:\\\\Users\\\\manoj\\\\Downloads\\\\keyword2tumor_type.csv\")\n",
    "\n",
    "# Display basic information about each CSV file\n",
    "print(\"Train CSV:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nTest CSV:\")\n",
    "print(test_df.info())\n",
    "print(\"\\nKeyword CSV:\")\n",
    "print(keyword_df.info())\n",
    "\n",
    "# Display sample data from each CSV file\n",
    "print(\"\\nSample Data from Train CSV:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nSample Data from Test CSV:\")\n",
    "print(test_df.head())\n",
    "print(\"\\nSample Data from Keyword CSV:\")\n",
    "print(keyword_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Predictions:\n",
      "   doc_id   prediction\n",
      "0       1        Brust\n",
      "1       3        Brust\n",
      "2       4  no evidence\n",
      "3       5  no evidence\n",
      "4       6  no evidence\n",
      "\n",
      "Test Predictions:\n",
      "   doc_id   prediction\n",
      "0       0  no evidence\n",
      "1       2        Brust\n",
      "2       7        Brust\n",
      "3      15        Brust\n",
      "4      16        Brust\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords and their corresponding tumor board types\n",
    "keywords = keyword_df['keyword'].tolist()\n",
    "tumor_types = keyword_df['tumor_type'].tolist()\n",
    "\n",
    "def extract_clean_text(html_file_path):\n",
    "    with open(html_file_path, 'r', encoding='latin1') as file:\n",
    "        content = file.read()\n",
    "        soup = BeautifulSoup(content, 'xml')  # Use XML parser\n",
    "        # Remove unwanted tags like <script> and <style>\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        return soup.get_text(separator=' ')\n",
    "\n",
    "# Classify HTML documents based on the presence of tumor board keywords\n",
    "def classify_html(html_text):\n",
    "    for keyword, tumor_type in zip(keywords, tumor_types):\n",
    "        if keyword.lower() in html_text.lower():\n",
    "            return tumor_type\n",
    "    return \"no evidence\"\n",
    "\n",
    "# Process train data\n",
    "train_predictions = []\n",
    "for index, row in train_df.iterrows():\n",
    "    html_file_path = os.path.join(\"C:\\\\Users\\\\manoj\\\\Downloads\\\\htmls\\\\htmls\", f\"{row['doc_id']}.html\")\n",
    "    html_text = extract_clean_text(html_file_path)\n",
    "    classification = classify_html(html_text)\n",
    "    train_predictions.append((row['doc_id'], classification))\n",
    "\n",
    "# Process test data\n",
    "test_predictions = []\n",
    "for index, row in test_df.iterrows():\n",
    "    html_file_path = os.path.join(\"C:\\\\Users\\\\manoj\\\\Downloads\\\\htmls\\\\htmls\", f\"{row['doc_id']}.html\")\n",
    "    html_text = extract_clean_text(html_file_path)\n",
    "    classification = classify_html(html_text)\n",
    "    test_predictions.append((row['doc_id'], classification))\n",
    "\n",
    "# Save predictions to CSV files\n",
    "train_predictions_df = pd.DataFrame(train_predictions, columns=['doc_id', 'prediction'])\n",
    "test_predictions_df = pd.DataFrame(test_predictions, columns=['doc_id', 'prediction'])\n",
    "\n",
    "train_predictions_df.to_csv(\"train_predictions.csv\", index=False)\n",
    "test_predictions_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "# Load train_predictions.csv and test_predictions.csv\n",
    "train_predictions_df = pd.read_csv(\"train_predictions.csv\")\n",
    "test_predictions_df = pd.read_csv(\"test_predictions.csv\")\n",
    "\n",
    "# Display the first few rows of each CSV file\n",
    "print(\"Train Predictions:\")\n",
    "print(train_predictions_df.head())\n",
    "\n",
    "print(\"\\nTest Predictions:\")\n",
    "print(test_predictions_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting HTML files for train predictions:\n",
      "\n",
      "Doc ID: 146, Prediction: Darm\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\146.html\n",
      "\n",
      "Doc ID: 22, Prediction: Brust\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\22.html\n",
      "\n",
      "Doc ID: 18, Prediction: Brust\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\18.html\n",
      "\n",
      "\n",
      "Inspecting HTML files for test predictions:\n",
      "\n",
      "Doc ID: 31, Prediction: Brust\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\31.html\n",
      "\n",
      "Doc ID: 147, Prediction: Darm\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\147.html\n",
      "\n",
      "Doc ID: 24, Prediction: no evidence\n",
      "HTML File Path: C:\\Users\\manoj\\Downloads\\htmls\\htmls\\24.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to inspect HTML files based on predictions\n",
    "def inspect_html_files(predictions_df, dataset_name):\n",
    "    print(f\"\\nInspecting HTML files for {dataset_name} predictions:\\n\")\n",
    "    for index, row in predictions_df.sample(3).iterrows():  # Selecting 3 random rows for inspection\n",
    "        doc_id = row['doc_id']\n",
    "        prediction = row['prediction']\n",
    "        html_file_path = f\"C:\\\\Users\\\\manoj\\\\Downloads\\\\htmls\\\\htmls\\\\{doc_id}.html\"\n",
    "        print(f\"Doc ID: {doc_id}, Prediction: {prediction}\")\n",
    "        # You can manually inspect the HTML file at html_file_path to verify if the prediction aligns with the content\n",
    "        # For example, you can open the HTML file in a web browser or a text editor\n",
    "        print(f\"HTML File Path: {html_file_path}\\n\")\n",
    "\n",
    "# Inspect HTML files for train predictions\n",
    "inspect_html_files(train_predictions_df, \"train\")\n",
    "\n",
    "# Inspect HTML files for test predictions\n",
    "inspect_html_files(test_predictions_df, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation:\n",
    "# How did you decide to handle this amount of data?\n",
    "# I chose to read the HTML content from each file individually using BeautifulSoup to extract clean text.\n",
    "# Since the amount of data is relatively small (100 train instances and 48 test instances), this approach should be manageable \n",
    "# without overwhelming system resources.\n",
    "\n",
    "# How did you decide to do feature engineering?\n",
    "# Feature engineering wasn't explicitly performed in this task as the main focus was on extracting relevant information from\n",
    "# the HTML content using keyword matching. The features were derived directly from the presence or absence of tumor board-related\n",
    "# keywords in the text.\n",
    "\n",
    "# How did you decide which models to try (if you decide to train any models)?\n",
    "# Since this task primarily involved classifying HTML content based on the presence of tumor board-related keywords, I chose a \n",
    "# simple keyword matching approach rather than training complex machine learning models. This decision was made considering the \n",
    "# interpretability and ease of implementation of the keyword-based classification method.\n",
    "\n",
    "# How did you perform validation of your model? What metrics did you measure?\n",
    "# Since no traditional machine learning model was trained, there was no separate validation process. However, during the\n",
    "# inspection of predictions, I manually verified the alignment of the predicted tumor board types with the content of the HTML\n",
    "# files. The primary metric measured was the accuracy of the keyword-based classification.\n",
    "\n",
    "# How do you expect your model to perform on test data (in terms of your metrics)?\n",
    "# I expect the keyword-based classification model to perform reasonably well on the test data, given that the same approach was \n",
    "# successful on the training data. However, the performance may vary depending on the quality and diversity of the HTML content\n",
    "# in the test set.\n",
    "\n",
    "# How fast will your algorithm perform, and how could you improve its performance if you would have more time?\n",
    "# The algorithm's performance largely depends on the number of HTML files to process and the complexity of the keyword matching.\n",
    "# Since the task involves reading and processing HTML content, the runtime may increase with a larger number of files. To improve\n",
    "# performance, optimizing the keyword matching algorithm or implementing parallel processing techniques could be considered.\n",
    "\n",
    "# How do you think you would be able to improve your algorithm if you would have more data?\n",
    "# With more data, the algorithm's performance could potentially improve by refining the list of tumor board-related keywords and\n",
    "# incorporating additional features derived from the HTML content, such as structural information or semantic analysis. \n",
    "# Furthermore, training more sophisticated machine learning models on labeled data could lead to better classification accuracy.\n",
    "\n",
    "# What potential issues do you see with your algorithm?\n",
    "# One potential issue with the keyword-based classification approach is its reliance on the presence of specific keywords in the \n",
    "# HTML content. If certain tumor board types are not adequately represented by the chosen keywords or if the keywords appear in \n",
    "# unrelated contexts, the algorithm's accuracy may suffer. Additionally, the algorithm may struggle with classifying HTML content\n",
    "# that deviates significantly from the expected format or structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
