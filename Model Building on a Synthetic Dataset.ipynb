{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEMhP_X6Vj2S"
   },
   "source": [
    "#**Model Building on a Synthetic Dataset**\n",
    "\n",
    "##**Assignment**\n",
    "The two synthetic datasets were generated using the same underlying data model. Your goal is to build a predictive model using the data in the training dataset to predict the withheld target values from the test set.\n",
    "\n",
    "You may use any tools available to you for this task. Ultimately, we will assess predictive accuracy on the test set using the mean squared error metric. You should produce the following:\n",
    "\n",
    "- A 1,000 x 1 text file containing 1 prediction per line for each record in the test dataset.\n",
    "- A brief writeup describing the techniques you used to generate the predictions. Details such as important features and your estimates of predictive performance are helpful here, though not strictly necessary.\n",
    "- (Optional) An implementable version of your model. What this would look like largely depends on the methods you used, but could include things like source code, a pickled Python object, a PMML file, etc. Please do not include any compiled executables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiJtKqEeVjtY"
   },
   "source": [
    "##**Data Description**\n",
    "We have provided two tab-delimited files along with these instructions:\n",
    "\n",
    "- codetest_train.txt: 5,000 records x 254 features + 1 target\n",
    "\n",
    "- codetest_test.txt : 1,000 records x 254 features\n",
    "\n",
    "\n",
    "\n",
    "###**Practicalities**\n",
    "The purpose of this test is to test your ability to write software to collect, normalize, store, analyze and visualize “real world” data. You may also use any tools or software on your computer, or that are freely available on the Internet. We prefer that you use simpler tools to more complex ones and that you are “lazy” in the sense of using third party APIs and libraries as much as possible. We encourage the reuse of code when appropriate. If you include code directly in your submission that was written by someone else other than commonly imported modules, please be sure to provide proper attribution, including a URL, text, author, etc. or other available information in the code comments.\n",
    "\n",
    "Do as much as you can, as well as you can. Prefer efficient, elegant solutions. Prefer scripted analysis to unrepeatable use of GUI tools. For data security and transfer time reasons, you have been given a relatively small data file. Prefer solutions that do not require the full data set to be stored in memory.\n",
    "\n",
    "There is certainly no requirement that you have previous experience working on these kinds of problems. Rather, we are looking for an ability to research and select the appropriate tools for an open-ended problem and implement something meaningful. We are also interested in your ability to work on a team, which means considering how to package and deliver your results in a way that makes it easy for others to review them. Undocumented code and data dumps are virtually useless; commented code and a clear writeup with elegant visuals are ideal.\n",
    "\n",
    "#### To download the dataset <a href=\"https://drive.google.com/drive/folders/1fTWdsoPQCXaqBwrwjZQkzb8t-aA0ijTJ?usp=sharing\"> Click here </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 255)\n",
      "(1000, 254)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data\n",
    "train_file_path = \"C:/Users/manoj/Downloads/codetest_train.txt\"\n",
    "train_data = pd.read_csv(train_file_path, delimiter='\\t')\n",
    "\n",
    "# Load the test data\n",
    "test_file_path = \"C:/Users/manoj/Downloads/codetest_test.txt\"\n",
    "test_data = pd.read_csv(test_file_path, delimiter='\\t')\n",
    "\n",
    "# Check the shape of the datasets\n",
    "print(train_data.shape)  # Should be (5000, 255)\n",
    "print(test_data.shape)   # Should be (1000, 254)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     target    f_0    f_1    f_2    f_3    f_4    f_5    f_6    f_7    f_8  \\\n",
      "0  3.066056 -0.653  0.255 -0.615 -1.833 -0.736    NaN  1.115 -0.171 -0.351   \n",
      "1 -1.910473  1.179 -0.093 -0.556  0.811 -0.468 -0.005 -0.116 -1.243  1.985   \n",
      "2  7.830711  0.181 -0.778 -0.919  0.113  0.887 -0.762  1.872 -1.709  0.135   \n",
      "3 -2.180862  0.745 -0.245 -1.343  1.163 -0.169 -0.151 -1.100  0.225  1.223   \n",
      "4  5.462784  1.217 -1.324 -0.958  0.448 -2.873 -0.856  0.603  0.763  0.020   \n",
      "\n",
      "   ...  f_244  f_245  f_246  f_247  f_248  f_249  f_250  f_251  f_252  f_253  \n",
      "0  ... -1.607 -1.400 -0.920 -0.198 -0.945 -0.573  0.170 -0.418 -1.244 -0.503  \n",
      "1  ...  1.282  0.032 -0.061    NaN -0.061 -0.302  1.281 -0.850  0.821 -0.260  \n",
      "2  ... -0.237 -0.660  1.073 -0.193  0.570 -0.267  1.435  1.332 -1.147  2.580  \n",
      "3  ...  0.709 -0.203 -0.136 -0.571  1.682  0.243 -0.381  0.613  1.033  0.400  \n",
      "4  ...  0.892 -0.433 -0.877  0.289  0.654  1.230  0.457 -0.754 -0.025 -0.931  \n",
      "\n",
      "[5 rows x 255 columns]\n",
      "25207\n",
      "4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manoj\\AppData\\Local\\Temp\\ipykernel_8788\\4216849631.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(train_data.applymap(lambda x: isinstance(x, (int, float))).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    True\n",
      "f_0       True\n",
      "f_1       True\n",
      "f_2       True\n",
      "f_3       True\n",
      "          ... \n",
      "f_249     True\n",
      "f_250     True\n",
      "f_251     True\n",
      "f_252     True\n",
      "f_253     True\n",
      "Length: 255, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the training data\n",
    "print(train_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(train_data.isnull().sum().sum())\n",
    "print(test_data.isnull().sum().sum())\n",
    "\n",
    "# Check for non-numeric values\n",
    "print(train_data.applymap(lambda x: isinstance(x, (int, float))).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate features and target variable in training data\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "# Identify and convert non-numeric columns to numeric (if possible) or drop them\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'object':\n",
    "        # Attempt to convert non-numeric values to numeric\n",
    "        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "\n",
    "# Fill missing values with SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(test_data)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Mean Squared Error: 14.8377\n",
      "Random Forest - Mean Squared Error: 13.3959\n",
      "Gradient Boosting - Mean Squared Error: 9.2339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models using cross-validation\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(f\"{name} - Mean Squared Error: {-scores.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For demonstration, let's assume Gradient Boosting performed the best\n",
    "best_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "best_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save predictions to a text file\n",
    "output_file_path = \"C:/Users/manoj/Downloads/predictions.txt\"\n",
    "np.savetxt(output_file_path, predictions, fmt='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report.\n",
    "\n",
    "#Techniques Used:\n",
    "\n",
    "#Data Preprocessing: Converted non-numeric values to numeric, filled missing values with column means, and standardized the features.\n",
    "#Model Selection: Experimented with Linear Regression, Random Forest, and Gradient Boosting. Selected Gradient Boosting based on cross-validation \n",
    "#results.\n",
    "#Evaluation Metric: Used Mean Squared Error (MSE) for model evaluation.\n",
    "#Important Features:\n",
    "\n",
    "#Feature importance can be extracted from the Gradient Boosting model, but this step is optional for now.\n",
    "#Predictive Performance:\n",
    "\n",
    "#Gradient Boosting achieved the lowest MSE during cross-validation, indicating it generalizes well on unseen data.\n",
    "#Future Work:\n",
    "\n",
    "#Further tuning of hyperparameters.\n",
    "#Experiment with additional models and ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "model_file_path = \"C:/Users/manoj/Downloads/best_model.pkl\"\n",
    "with open(model_file_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
